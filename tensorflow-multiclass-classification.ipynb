{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 125973 rows.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #path = get_file('kddcup.data.gz', origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz')\n",
    "    #path = get_file('kddcup.data_10_percent.gz', origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
    "    path = './nslkdd/KDDTrain+.txt'\n",
    "except:\n",
    "    print('Error downloading')\n",
    "    raise\n",
    "    \n",
    "df = pd.read_csv(path, header=None)\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "#df = df.sample(frac=0.01, replace=False) # Uncomment this line to sample only 1% of the dataset\n",
    "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "\n",
    "df.columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'outcome',\n",
    "    'difficulty_rating'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "    \n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "outcome\n",
      "back                 956\n",
      "buffer_overflow       30\n",
      "ftp_write              8\n",
      "guess_passwd          53\n",
      "imap                  11\n",
      "ipsweep             3599\n",
      "land                  18\n",
      "loadmodule             9\n",
      "multihop               7\n",
      "neptune            41214\n",
      "nmap                1493\n",
      "normal             67343\n",
      "perl                   3\n",
      "phf                    4\n",
      "pod                  201\n",
      "portsweep           2931\n",
      "rootkit               10\n",
      "satan               3633\n",
      "smurf               2646\n",
      "spy                    2\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "warezmaster           20\n",
      "Name: outcome, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Now encode the feature vector\n",
    "\n",
    "encode_numeric_zscore(df, 'duration')\n",
    "encode_text_dummy(df, 'protocol_type')\n",
    "encode_text_dummy(df, 'service')\n",
    "encode_text_dummy(df, 'flag')\n",
    "encode_numeric_zscore(df, 'src_bytes')\n",
    "encode_numeric_zscore(df, 'dst_bytes')\n",
    "encode_text_dummy(df, 'land')\n",
    "encode_numeric_zscore(df, 'wrong_fragment')\n",
    "encode_numeric_zscore(df, 'urgent')\n",
    "encode_numeric_zscore(df, 'hot')\n",
    "encode_numeric_zscore(df, 'num_failed_logins')\n",
    "encode_text_dummy(df, 'logged_in')\n",
    "encode_numeric_zscore(df, 'num_compromised')\n",
    "encode_numeric_zscore(df, 'root_shell')\n",
    "encode_numeric_zscore(df, 'su_attempted')\n",
    "encode_numeric_zscore(df, 'num_root')\n",
    "encode_numeric_zscore(df, 'num_file_creations')\n",
    "encode_numeric_zscore(df, 'num_shells')\n",
    "encode_numeric_zscore(df, 'num_access_files')\n",
    "encode_numeric_zscore(df, 'num_outbound_cmds')\n",
    "encode_text_dummy(df, 'is_host_login')\n",
    "encode_text_dummy(df, 'is_guest_login')\n",
    "encode_numeric_zscore(df, 'count')\n",
    "encode_numeric_zscore(df, 'srv_count')\n",
    "encode_numeric_zscore(df, 'serror_rate')\n",
    "encode_numeric_zscore(df, 'srv_serror_rate')\n",
    "encode_numeric_zscore(df, 'rerror_rate')\n",
    "encode_numeric_zscore(df, 'srv_rerror_rate')\n",
    "encode_numeric_zscore(df, 'same_srv_rate')\n",
    "encode_numeric_zscore(df, 'diff_srv_rate')\n",
    "encode_numeric_zscore(df, 'srv_diff_host_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_count')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_count')\n",
    "encode_numeric_zscore(df, 'dst_host_same_srv_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_diff_srv_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_same_src_port_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_diff_host_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_serror_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_serror_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_rerror_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_rerror_rate')\n",
    "\n",
    "# display 5 rows\n",
    "\n",
    "df.dropna(inplace=True,axis=1)\n",
    "df[0:5]\n",
    "# This is the numeric feature vector, as it goes to the neural net\n",
    "\n",
    "df_trainer = None\n",
    "\n",
    "print(df['outcome'].nunique())\n",
    "print(df.groupby('outcome')['outcome'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 0\n",
    "while num_classes != 23: ###I need a sample that contains all the types of attacks, otherwise I cannot classify some of them\n",
    "\n",
    "    df_trainer = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "    \n",
    "    # Convert to numpy - Classification\n",
    "    x_columns = df_trainer.columns.drop(['outcome','difficulty_rating'])\n",
    "    x = df_trainer[x_columns].values\n",
    "    dummies = pd.get_dummies(df_trainer['outcome']) # Classification\n",
    "    #dummies = pd.get_dummies(df['outcome']) # Classification\n",
    "\n",
    "    outcomes = dummies.columns\n",
    "    num_classes = len(outcomes)\n",
    "    y = dummies.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome\n",
      "back                 99\n",
      "buffer_overflow       2\n",
      "ftp_write             2\n",
      "guess_passwd          6\n",
      "imap                  1\n",
      "ipsweep             338\n",
      "land                  2\n",
      "loadmodule            1\n",
      "multihop              2\n",
      "neptune            4120\n",
      "nmap                134\n",
      "normal             6709\n",
      "perl                  1\n",
      "phf                   1\n",
      "pod                  22\n",
      "portsweep           301\n",
      "rootkit               1\n",
      "satan               395\n",
      "smurf               264\n",
      "spy                   1\n",
      "teardrop            100\n",
      "warezclient          93\n",
      "warezmaster           2\n",
      "Name: outcome, dtype: int64\n",
      "       duration  src_bytes  dst_bytes  wrong_fragment    urgent        hot  \\\n",
      "38085 -0.110249  -0.007762  -0.004919       -0.089486 -0.007736  -0.095075   \n",
      "25407 -0.110249  -0.007710  -0.004724       -0.089486 -0.007736  -0.095075   \n",
      "53643 -0.110249  -0.007762  -0.004919       -0.089486 -0.007736  -0.095075   \n",
      "19883 -0.109865  -0.007548  -0.004309       -0.089486 -0.007736  12.928372   \n",
      "49816 -0.110249  -0.007755  -0.004887       -0.089486 -0.007736  -0.095075   \n",
      "\n",
      "       num_failed_logins  num_compromised  root_shell  su_attempted  ...  \\\n",
      "38085          -0.027023        -0.011664   -0.036652     -0.024436  ...   \n",
      "25407          -0.027023        -0.011664   -0.036652     -0.024436  ...   \n",
      "53643          -0.027023        -0.011664   -0.036652     -0.024436  ...   \n",
      "19883          -0.027023        -0.011664   -0.036652     -0.024436  ...   \n",
      "49816          -0.027023        -0.011664   -0.036652     -0.024436  ...   \n",
      "\n",
      "       flag-SF  flag-SH  land-0  land-1  logged_in-0  logged_in-1  \\\n",
      "38085        0        0       1       0            1            0   \n",
      "25407        1        0       1       0            0            1   \n",
      "53643        0        0       1       0            1            0   \n",
      "19883        1        0       1       0            0            1   \n",
      "49816        1        0       1       0            1            0   \n",
      "\n",
      "       is_host_login-0  is_host_login-1  is_guest_login-0  is_guest_login-1  \n",
      "38085                1                0                 1                 0  \n",
      "25407                1                0                 1                 0  \n",
      "53643                1                0                 1                 0  \n",
      "19883                1                0                 0                 1  \n",
      "49816                1                0                 1                 0  \n",
      "\n",
      "[5 rows x 127 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_trainer.groupby('outcome')['outcome'].count())\n",
    "print(df_trainer.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=x.shape[1], activation='linear'))\n",
    "    model.add(Dense(32, input_dim=x.shape[1], activation='linear'))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8.96907568\n",
      "Iteration 2, loss = 1.11829688\n",
      "Iteration 3, loss = 0.58966816\n",
      "Iteration 4, loss = 0.43957917\n",
      "Iteration 5, loss = 0.36132636\n",
      "Iteration 6, loss = 0.31517468\n",
      "Iteration 7, loss = 0.27995554\n",
      "Iteration 8, loss = 0.25246804\n",
      "Iteration 9, loss = 0.22802262\n",
      "Iteration 10, loss = 0.20742925\n",
      "Iteration 11, loss = 0.18633664\n",
      "Iteration 12, loss = 0.16655498\n",
      "Iteration 13, loss = 0.14845235\n",
      "Iteration 14, loss = 0.13196549\n",
      "Iteration 15, loss = 0.11904668\n",
      "Iteration 16, loss = 0.10758298\n",
      "Iteration 17, loss = 0.09947519\n",
      "Iteration 18, loss = 0.09183095\n",
      "Iteration 19, loss = 0.08630136\n",
      "Iteration 20, loss = 0.08131677\n",
      "Iteration 21, loss = 0.07597667\n",
      "Iteration 22, loss = 0.07311726\n",
      "Iteration 23, loss = 0.06885225\n",
      "Iteration 24, loss = 0.06636278\n",
      "Iteration 25, loss = 0.06321704\n",
      "Iteration 26, loss = 0.06097287\n",
      "Iteration 27, loss = 0.05912293\n",
      "Iteration 28, loss = 0.05930900\n",
      "Iteration 29, loss = 0.05585196\n",
      "Iteration 30, loss = 0.05373133\n",
      "Iteration 31, loss = 0.05245718\n",
      "Iteration 32, loss = 0.05078807\n",
      "Iteration 33, loss = 0.04940984\n",
      "Iteration 34, loss = 0.04809896\n",
      "Iteration 35, loss = 0.04726576\n",
      "Iteration 36, loss = 0.04651320\n",
      "Iteration 37, loss = 0.04585931\n",
      "Iteration 38, loss = 0.04359117\n",
      "Iteration 39, loss = 0.04433767\n",
      "Iteration 40, loss = 0.04208390\n",
      "Iteration 41, loss = 0.04030223\n",
      "Iteration 42, loss = 0.04223040\n",
      "Iteration 43, loss = 0.04055794\n",
      "Iteration 44, loss = 0.03862610\n",
      "Iteration 45, loss = 0.03789112\n",
      "Iteration 46, loss = 0.03952404\n",
      "Iteration 47, loss = 0.03740038\n",
      "Iteration 48, loss = 0.03671500\n",
      "Iteration 49, loss = 0.03557230\n",
      "Iteration 50, loss = 0.03572068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(64, 64), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=50, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64,64,),max_iter=50,verbose=True)\n",
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        22\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.92      1.00      0.96        80\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       1.00      1.00      1.00      1050\n",
      "          10       0.94      0.86      0.90        36\n",
      "          11       0.98      0.99      0.99      1659\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       1.00      1.00      1.00         6\n",
      "          15       1.00      0.96      0.98        77\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       1.00      0.93      0.97       105\n",
      "          18       0.98      0.95      0.97        64\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       1.00      1.00      1.00        26\n",
      "          21       0.78      0.37      0.50        19\n",
      "          22       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      3150\n",
      "   macro avg       0.46      0.44      0.45      3150\n",
      "weighted avg       0.98      0.99      0.98      3150\n",
      " samples avg       0.99      0.99      0.99      3150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chris\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\chris\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = mlp.predict(x_test)\n",
    "#print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9447 samples, validate on 3150 samples\n",
      "Epoch 1/50\n",
      "9447/9447 - 11s - loss: 0.6644 - accuracy: 0.8667 - val_loss: 0.2356 - val_accuracy: 0.9419\n",
      "Epoch 2/50\n",
      "9447/9447 - 10s - loss: 0.1782 - accuracy: 0.9488 - val_loss: 0.1666 - val_accuracy: 0.9597\n",
      "Epoch 3/50\n",
      "9447/9447 - 11s - loss: 0.1330 - accuracy: 0.9596 - val_loss: 0.1401 - val_accuracy: 0.9632\n",
      "Epoch 4/50\n",
      "9447/9447 - 10s - loss: 0.1122 - accuracy: 0.9638 - val_loss: 0.1282 - val_accuracy: 0.9717\n",
      "Epoch 5/50\n",
      "9447/9447 - 9s - loss: 0.0953 - accuracy: 0.9695 - val_loss: 0.1316 - val_accuracy: 0.9711\n",
      "Epoch 6/50\n",
      "9447/9447 - 9s - loss: 0.0840 - accuracy: 0.9730 - val_loss: 0.1483 - val_accuracy: 0.9752\n",
      "Epoch 7/50\n",
      "9447/9447 - 11s - loss: 0.0774 - accuracy: 0.9758 - val_loss: 0.1771 - val_accuracy: 0.9765\n",
      "Epoch 8/50\n",
      "9447/9447 - 9s - loss: 0.0705 - accuracy: 0.9775 - val_loss: 0.2099 - val_accuracy: 0.9737\n",
      "Epoch 9/50\n",
      "9447/9447 - 11s - loss: 0.0694 - accuracy: 0.9776 - val_loss: 0.2418 - val_accuracy: 0.9781\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x250aef2bb48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "estimator.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "pred = estimator.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  9 11 ...  9 11 11]\n",
      "Validation score: 0.9780952380952381\n"
     ]
    }
   ],
   "source": [
    "y_eval = np.argmax(y_test, axis=1)\n",
    "print(y_eval)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop(['outcome','difficulty_rating'])\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['outcome']) # Classification\n",
    "outcomes = dummies.columns\n",
    "num_classes = len(outcomes)\n",
    "y = dummies.values\n",
    "\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score for entire dataset: 0.9788446730648631\n"
     ]
    }
   ],
   "source": [
    "y_eval_full = np.argmax(y, axis=1)\n",
    "full_pred = estimator.predict(x)\n",
    "score = metrics.accuracy_score(y_eval_full, full_pred)\n",
    "print(\"Validation score for entire dataset: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
