{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def get_remote_dataset(URL,header=None):\n",
    "    try:\n",
    "        path = get_file(URL.split('/')[-1], origin=URL)\n",
    "    except:\n",
    "        print('Error downloading remote dataset.')\n",
    "        raise\n",
    "    return pd.read_csv(path, header=header)\n",
    "\n",
    "def get_local_dataset(PATH, header=None):\n",
    "    try:\n",
    "        df = pd.read_csv(PATH,header=header)\n",
    "    except:\n",
    "        print('Error loading local dataset.')\n",
    "        raise\n",
    "    df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "    print('Read {} rows.'.format(len(df)))\n",
    "    return df\n",
    "\n",
    "def set_KDD_columns(kdd_df):\n",
    "    kdd_df.columns = [\n",
    "        'duration',\n",
    "        'protocol_type',\n",
    "        'service',\n",
    "        'flag',\n",
    "        'src_bytes',\n",
    "        'dst_bytes',\n",
    "        'land',\n",
    "        'wrong_fragment',\n",
    "        'urgent',\n",
    "        'hot',\n",
    "        'num_failed_logins',\n",
    "        'logged_in',\n",
    "        'num_compromised',\n",
    "        'root_shell',\n",
    "        'su_attempted',\n",
    "        'num_root',\n",
    "        'num_file_creations',\n",
    "        'num_shells',\n",
    "        'num_access_files',\n",
    "        'num_outbound_cmds',\n",
    "        'is_host_login',\n",
    "        'is_guest_login',\n",
    "        'count',\n",
    "        'srv_count',\n",
    "        'serror_rate',\n",
    "        'srv_serror_rate',\n",
    "        'rerror_rate',\n",
    "        'srv_rerror_rate',\n",
    "        'same_srv_rate',\n",
    "        'diff_srv_rate',\n",
    "        'srv_diff_host_rate',\n",
    "        'dst_host_count',\n",
    "        'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate',\n",
    "        'dst_host_diff_srv_rate',\n",
    "        'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate',\n",
    "        'dst_host_serror_rate',\n",
    "        'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate',\n",
    "        'dst_host_srv_rerror_rate',\n",
    "        'outcome',\n",
    "        'difficulty_rating'\n",
    "    ]\n",
    "    #return kdd_df\n",
    "\n",
    "def set_bin_class(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if row['outcome'].split('.')[0] != 'normal':\n",
    "            df.at[i, 'outcome'] = 'anomaly'\n",
    "\n",
    "def set_multi_class(df):\n",
    "    ### THIS WILL ONLY WORK WITH THE ENTIRE KDD DATASET. WILL NOT WORK WITH NSL-KDD SET!!! ###\n",
    "    ### THIS IS YOUR ONLY WARNING!!!###\n",
    "    DOS_TYPES = ('back','land','neptune','pod','smurf','teardrop')\n",
    "    U2R_TYPES = ('buffer_overflow','loadmodule','perl','rootkit')\n",
    "    R2L_TYPES = ('ftp_write','guess_passwd','imap','multihop','phf','spy','warezclient','warezmaster')\n",
    "    PROBE_TYPES = ('ipsweep','nmap','portsweep','satan')\n",
    "            \n",
    "    for i, row in df.iterrows():\n",
    "        val = 'normal'\n",
    "        old_val = row['outcome'].split('.')[0]\n",
    "        if old_val in DOS_TYPES:\n",
    "            val = 'dos'\n",
    "        elif old_val in U2R_TYPES:\n",
    "            val = 'u2r'\n",
    "        elif old_val in R2L_TYPES:\n",
    "            val = 'r2l'\n",
    "        elif old_val in PROBE_TYPES:\n",
    "            val = 'probe'\n",
    "        df.at[i,'outcome'] = val \n",
    "\n",
    "def encode_zscore(df, name, mean=None, std_dev=None):\n",
    "    '''Encode numeric values as zscore'''\n",
    "    if mean == None:\n",
    "        mean = df[name].mean()\n",
    "    if std_dev == None:\n",
    "        std_dev = df[name].std()\n",
    "    df[name] = (df[name] - mean) / std_dev\n",
    "\n",
    "def encode_text(df, name):\n",
    "    '''Encode text values to binary dummy values (i.e. red,blue is [0,1] or [1,0])'''\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        new_name = f\"{name}-{x}\"\n",
    "        df[new_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "def reencode_dataset(df):\n",
    "    '''Takes a KDD pandas dataframe and transforms the data by \n",
    "    changing numeric columns to zscore, and text columns to \n",
    "    dummy values'''\n",
    "\n",
    "    encode_zscore(df, 'duration')\n",
    "    encode_text(df, 'protocol_type')\n",
    "    encode_text(df, 'service')\n",
    "    encode_text(df, 'flag')\n",
    "    encode_zscore(df, 'src_bytes')\n",
    "    encode_zscore(df, 'dst_bytes')\n",
    "    encode_text(df, 'land')\n",
    "    encode_zscore(df, 'wrong_fragment')\n",
    "    encode_zscore(df, 'urgent')\n",
    "    encode_zscore(df, 'hot')\n",
    "    encode_zscore(df, 'num_failed_logins')\n",
    "    encode_text(df, 'logged_in')\n",
    "    encode_zscore(df, 'num_compromised')\n",
    "    encode_zscore(df, 'root_shell')\n",
    "    encode_zscore(df, 'su_attempted')\n",
    "    encode_zscore(df, 'num_root')\n",
    "    encode_zscore(df, 'num_file_creations')\n",
    "    encode_zscore(df, 'num_shells')\n",
    "    encode_zscore(df, 'num_access_files')\n",
    "    encode_zscore(df, 'num_outbound_cmds')\n",
    "    encode_text(df, 'is_host_login')\n",
    "    encode_text(df, 'is_guest_login')\n",
    "    encode_zscore(df, 'count')\n",
    "    encode_zscore(df, 'srv_count')\n",
    "    encode_zscore(df, 'serror_rate')\n",
    "    encode_zscore(df, 'srv_serror_rate')\n",
    "    encode_zscore(df, 'rerror_rate')\n",
    "    encode_zscore(df, 'srv_rerror_rate')\n",
    "    encode_zscore(df, 'same_srv_rate')\n",
    "    encode_zscore(df, 'diff_srv_rate')\n",
    "    encode_zscore(df, 'srv_diff_host_rate')\n",
    "    encode_zscore(df, 'dst_host_count')\n",
    "    encode_zscore(df, 'dst_host_srv_count')\n",
    "    encode_zscore(df, 'dst_host_same_srv_rate')\n",
    "    encode_zscore(df, 'dst_host_diff_srv_rate')\n",
    "    encode_zscore(df, 'dst_host_same_src_port_rate')\n",
    "    encode_zscore(df, 'dst_host_srv_diff_host_rate')\n",
    "    encode_zscore(df, 'dst_host_serror_rate')\n",
    "    encode_zscore(df, 'dst_host_srv_serror_rate')\n",
    "    encode_zscore(df, 'dst_host_rerror_rate')\n",
    "    encode_zscore(df, 'dst_host_srv_rerror_rate')\n",
    "\n",
    "    #return df\n",
    "\n",
    "def generate_training_set(df, num_outcomes,frac=0.1):\n",
    "    '''This doesn't work well right now. FIX ME!!!'''\n",
    "    while True:\n",
    "        df_train = df.sample(frac=frac, replace=False)\n",
    "        dummies = pd.get_dummies(df_train['outcome'])\n",
    "        if len(dummies.columns) != num_outcomes:\n",
    "            continue\n",
    "        x_columns = df_train.columns.drop(['outcome','difficulty_rating'])\n",
    "        x = df_train[x_columns].values\n",
    "        y = dummies.values\n",
    "        break\n",
    "\n",
    "    print(df_train.groupby('outcome')['outcome'].count())\n",
    "    print(df_train.head())\n",
    "    print(df_train.columns)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def build_classifier(x, y, hidden_layers=[8], activation='relu', batch_size=None, verbose=1):\n",
    "    def baseline_model():\n",
    "        model = Sequential()\n",
    "        for layer in hidden_layers:\n",
    "            model.add(Dense(layer, activation=activation))\n",
    "        model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    estimator = KerasClassifier(build_fn=baseline_model, batch_size=batch_size, verbose=verbose)\n",
    "    return estimator\n",
    "\n",
    "def run_tf():\n",
    "    ###acquire and process dataset\n",
    "    df = get_local_dataset('./nslkdd/KDDTest+.txt')\n",
    "    #df_train = get_local_dataset('./nslkdd/KDDTrain+.txt')\n",
    "    set_KDD_columns(df)\n",
    "    reencode_dataset(df)\n",
    "    #set_bin_class(df)\n",
    "    df.dropna(inplace=True, axis=1)\n",
    "    x, y = generate_training_set(df,df['outcome'].nunique(),frac=1.0)\n",
    "\n",
    "    ###build MLP model\n",
    "    estimator = build_classifier(x, y, hidden_layers=[64,64])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    ###train model\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    estimator.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=1,epochs=100)\n",
    "\n",
    "    ###evaluate model\n",
    "    pred = estimator.predict(x_test)\n",
    "    y_eval = np.argmax(y_test, axis=1)\n",
    "    print(y_eval)\n",
    "    score = metrics.accuracy_score(y_eval, pred)\n",
    "    print(\"Validation score: {}\".format(score))\n",
    "\n",
    "def run_tf_full():\n",
    "    df_test = get_local_dataset('./nslkdd/KDDTest+.txt')\n",
    "    set_KDD_columns(df_test)\n",
    "    reencode_dataset(df_test)\n",
    "    set_bin_class(df_test)\n",
    "    for col in ('service-aol','service-harvest','service-http_2784','service-http_8001','service-red_i','service-urh_i'):\n",
    "        df_test[col] = 0    \n",
    "    df_test.fillna(value=0, inplace=True, axis=1)\n",
    "\n",
    "    x_final, y_final = generate_training_set(df_test, df_test['outcome'].nunique(), frac=1.0)\n",
    "    \n",
    "    df_train = get_local_dataset('./nslkdd/KDDTrain+.txt')\n",
    "    set_KDD_columns(df_train)\n",
    "    reencode_dataset(df_train)\n",
    "    set_bin_class(df_train)\n",
    "    df_train.fillna(value=0, inplace=True, axis=1)\n",
    "\n",
    "    df_test = df_test[df_train.columns] #fixes column ordering (does that matter? probably)\n",
    "    #print(df_test.columns)\n",
    "    #print(df_train.columns)\n",
    "    \n",
    "    x, y = generate_training_set(df_train, df_train['outcome'].nunique(), frac=0.1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "\n",
    "    #for col in df_train.columns:\n",
    "    #    if col not in df_test.columns:\n",
    "    #        print(col)\n",
    "\n",
    "    estimator = build_classifier(x_train, y_train, hidden_layers=[128,128,128,128,128,128,128,128,128,128])\n",
    "\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, verbose=1, mode='auto')\n",
    "    \n",
    "    estimator.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=1,epochs=100)\n",
    "\n",
    "    ###evaluate model\n",
    "    pred = estimator.predict(x_final)\n",
    "    y_eval = np.argmax(y_final, axis=1)\n",
    "    print(y_eval)\n",
    "    score = metrics.accuracy_score(y_eval, pred)\n",
    "    print(\"Validation score: {}\".format(score))\n",
    "\n",
    "def run_sklearn():\n",
    "    df = get_local_dataset('./nslkdd/KDDTrain+.txt')\n",
    "    df = set_KDD_columns(df)\n",
    "    df = reencode_dataset(df)\n",
    "    df.dropna(inplace=True, axis=1)\n",
    "    x, y = generate_training_set(df,df['outcome'].nunique())\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(64,64,),max_iter=50,verbose=True)\n",
    "    mlp.fit(x_train, y_train)\n",
    "\n",
    "    predictions = mlp.predict(x_test)\n",
    "    #print(confusion_matrix(y_test, predictions))\n",
    "    print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 22544 rows.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = get_local_dataset('./nslkdd/KDDTest+.txt')\n",
    "set_KDD_columns(df)\n",
    "set_bin_class(df)\n",
    "control = df\n",
    "reencode_dataset(df)\n",
    "\n",
    "df.dropna(inplace=True, axis=1)\n",
    "\n",
    "#x, y = generate_training_set(df,df['outcome'].nunique(), frac=1.0)\n",
    "\n",
    "#model = KMeans(n_jobs=-1).fit(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style='ticks')\n",
    "\n",
    "path = './figs/'\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "def build_scatterplots():\n",
    "    for i in (df.columns.drop('outcome')):\n",
    "        fig = sns.pairplot(df, x_vars=df.columns.drop('outcome'), y_vars=i, hue='outcome')\n",
    "        fig.savefig(path + 'fig-' + i)\n",
    "        plt.close(fig.fig)\n",
    "#fig = sns.pairplot(df, vars = ['duration','src_bytes','dst_bytes','wrong_fragment','urgent','hot'], hue='outcome', height=2, aspect=1, dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "    \n",
    "def run_process_pool(jobs=1):\n",
    "    def scatterfig(df, x, y, hue, path):\n",
    "        fig = sns.pairplot(df, vars = [x,y], hue = hue)\n",
    "        fig.savefig(path + 'figure- ' + x + 'vs' + y)\n",
    "        plt.close(fig.fig)\n",
    "    with multiprocessing.Pool(jobs) as pool:\n",
    "        for i in df.columns.drop('outcome'):\n",
    "            for j in df.columns.drop('outcome'):\n",
    "                pool.apply_async(scatterfig, args = (df,i,j,'outcome',path,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process_pool(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_scatterplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
